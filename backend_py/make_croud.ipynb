{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"./sample_data.csv\")\n",
    "# ユニコード正規化とアルファベットの小文字統一\n",
    "df.text = df.text.str.normalize(\"NFKC\").str.lower()\n",
    "# 改行コードを取り除く\n",
    "df.text = df.text.str.replace(\"\\n\", \" \")\n",
    "# エラーになる文字があるので取り除く (ライブドアニュースコーパス使う場合だけの処理。普通は不要)\n",
    "df.text = df.text.str.replace(\"\\u2028\", \"\")\n",
    "\n",
    "# 辞書のパス取得\n",
    "dicdir = subprocess.run([\"mecab-config\", \"--dicdir\"], capture_output=True, text=True).stdout.strip()\n",
    "tagger = MeCab.Tagger(f\"-d {dicdir}/ipadic\")\n",
    "\n",
    "# ひらがなのみの文字列にマッチする正規表現\n",
    "kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "\n",
    "# 分かち書きして、ひらがな以外を含む 名詞/動詞/形容詞 を返す関数\n",
    "def mecab_tokenizer(text):\n",
    "    # 分かち書き\n",
    "    parsed_lines = tagger.parse(text).splitlines()[:-1]\n",
    "    surfaces = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    features = [l.split('\\t')[1] for l in parsed_lines]\n",
    "    # 原型を取得\n",
    "    bases = [f.split(',')[6] for f in features]\n",
    "    # 品詞を取得\n",
    "    pos = [f.split(',')[0] for f in features]\n",
    "    # 各単語を原型に変換する\n",
    "    token_list = [b if b != '*' else s for s, b in zip(surfaces, bases)]\n",
    "    # 名詞/動詞/形容詞に絞り込み\n",
    "    target_pos = [\"名詞\", \"動詞\", \"形容詞\"]\n",
    "    token_list = [t for t, p in zip(token_list, pos) if (p in target_pos)]\n",
    "    # ひらがなのみの単語を除く\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "    # アルファベットを小文字に統一\n",
    "    token_list = [t.lower() for t in token_list]\n",
    "    # 半角スペースを挟んで結合する。\n",
    "    result = \" \".join(token_list)\n",
    "    # 念のためもう一度ユニコード正規化\n",
    "    result = unicodedata.normalize(\"NFKC\", result)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析\n",
    "df[\"tokens\"] = df.text.apply(mecab_tokenizer)\n",
    "# wordcloud入力データ用に連結する\n",
    "text_data = \" \".join(df[\"tokens\"])\n",
    "\n",
    "# ワードクラウドのオブジェクト生成\n",
    "wc = WordCloud(\n",
    "        font_path=\"/Library/Fonts/Arial Unicode.ttf\",  # 日本語フォントファイル\n",
    "        width=600,  # 幅\n",
    "        height=400,  # 高さ\n",
    "        prefer_horizontal=0.9,  # 横書きで配置することを試す確率 (デフォルト0.9)\n",
    "        background_color='white',  # 背景色\n",
    "        include_numbers=False,  # 数値だけの単語を含まない\n",
    "        colormap='tab20',  # 文字色のカラーマップ指定\n",
    "        regexp=r\"\\w{2,}\",  # 2文字以上の単語のみ含む\n",
    "        relative_scaling=1,  # 頻度のみで文字サイズを決める\n",
    "        collocations=False,  # bi-gramを考慮しない\n",
    "        max_font_size=60,  # 最大フォントサイズ\n",
    "        random_state=42,  # 乱数の初期値\n",
    "    ).generate(text_data)\n",
    "\n",
    "# この時点で作成できたwordcloudを確認する場合は以下の関数を実行。\n",
    "ex_text = wc.to_svg()\n",
    "# 出力省略\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37c59007b2efae7ece77d1d257bf5dac39310acb26ad458cba64f2540b5dde3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
